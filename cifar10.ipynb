{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMVo0aCYcMqheWwJJg89pZg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OliverNakamoto/TDT-4173-2023-task-1/blob/master/cifar10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pathlib\n",
        "import random\n",
        "\n",
        "# Allow torch/cudnn to optimize/analyze the input/output shape of convolutions\n",
        "# To optimize forward/backward pass.\n",
        "# This will increase model throughput for fixed input shape to the network\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "# Cudnn is not deterministic by default. Set this to True if you want\n",
        "# to be sure to reproduce your results\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "\n",
        "def set_seed(seed: int):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    random.seed(seed)\n",
        "\n",
        "\n",
        "def to_cuda(elements):\n",
        "    \"\"\"\n",
        "    Transfers every object in elements to GPU VRAM if available.\n",
        "    elements can be a object or list/tuple of objects\n",
        "    \"\"\"\n",
        "\n",
        "    device = get_device()\n",
        "    if type(elements) == tuple or type(elements) == list:\n",
        "        return [x.to(device) for x in elements]\n",
        "    return elements.to(device)\n",
        "\n",
        "\n",
        "def get_device():\n",
        "    device = (\n",
        "        \"cuda\"\n",
        "        if torch.cuda.is_available()\n",
        "        else \"mps\"\n",
        "        if torch.backends.mps.is_available()\n",
        "        else \"cpu\"\n",
        "    )\n",
        "    return device\n",
        "\n",
        "\n",
        "def save_checkpoint(\n",
        "    state_dict: dict, filepath: pathlib.Path, is_best: bool, max_keep: int = 1\n",
        "):\n",
        "    \"\"\"\n",
        "    Saves state_dict to filepath. Deletes old checkpoints as time passes.\n",
        "    If is_best is toggled, saves a checkpoint to best.ckpt\n",
        "    \"\"\"\n",
        "    filepath.parent.mkdir(exist_ok=True, parents=True)\n",
        "    list_path = filepath.parent.joinpath(\"latest_checkpoint\")\n",
        "    torch.save(state_dict, filepath)\n",
        "    if is_best:\n",
        "        torch.save(state_dict, filepath.parent.joinpath(\"best.ckpt\"))\n",
        "    previous_checkpoints = get_previous_checkpoints(filepath.parent)\n",
        "    if filepath.name not in previous_checkpoints:\n",
        "        previous_checkpoints = [filepath.name] + previous_checkpoints\n",
        "    if len(previous_checkpoints) > max_keep:\n",
        "        for ckpt in previous_checkpoints[max_keep:]:\n",
        "            path = filepath.parent.joinpath(ckpt)\n",
        "            if path.exists():\n",
        "                path.unlink()\n",
        "    previous_checkpoints = previous_checkpoints[:max_keep]\n",
        "    with open(list_path, \"w\") as fp:\n",
        "        fp.write(\"\\n\".join(previous_checkpoints))\n",
        "\n",
        "\n",
        "def get_previous_checkpoints(directory: pathlib.Path) -> list:\n",
        "    assert directory.is_dir()\n",
        "    list_path = directory.joinpath(\"latest_checkpoint\")\n",
        "    list_path.touch(exist_ok=True)\n",
        "    with open(list_path) as fp:\n",
        "        ckpt_list = fp.readlines()\n",
        "    return [_.strip() for _ in ckpt_list]\n",
        "\n",
        "\n",
        "def load_best_checkpoint(directory: pathlib.Path):\n",
        "    filepath = directory.joinpath(\"best.ckpt\")\n",
        "    if not filepath.is_file():\n",
        "        return None\n",
        "    return torch.load(directory.joinpath(\"best.ckpt\"))\n",
        "\n",
        "\n",
        "def plot_loss(\n",
        "    loss_dict: dict, label: str = None, npoints_to_average=1, plot_variance=True\n",
        "):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        loss_dict: a dictionary where keys are the global step and values are the given loss / accuracy\n",
        "        label: a string to use as label in plot legend\n",
        "        npoints_to_average: Number of points to average plot\n",
        "    \"\"\"\n",
        "    global_steps = list(loss_dict.keys())\n",
        "    loss = [item.cpu().numpy() if isinstance(item, torch.Tensor) else item for item in loss_dict.values()]\n",
        "    #loss = list([item.cpu().numpy() for item in loss_dict.values()])\n",
        "    if npoints_to_average == 1 or not plot_variance:\n",
        "        plt.plot(global_steps, loss, label=label)\n",
        "        return\n",
        "\n",
        "    npoints_to_average = 10\n",
        "    num_points = len(loss) // npoints_to_average\n",
        "    mean_loss = []\n",
        "    loss_std = []\n",
        "    steps = []\n",
        "    for i in range(num_points):\n",
        "        points = loss[i * npoints_to_average : (i + 1) * npoints_to_average]\n",
        "        step = global_steps[i * npoints_to_average + npoints_to_average // 2]\n",
        "        mean_loss.append(np.mean(points))\n",
        "        loss_std.append(np.std(points))\n",
        "        steps.append(step)\n",
        "    plt.plot(steps, mean_loss, label=f\"{label} (mean over {npoints_to_average} steps)\")\n",
        "    plt.fill_between(\n",
        "        steps,\n",
        "        np.array(mean_loss) - np.array(loss_std),\n",
        "        np.array(mean_loss) + loss_std,\n",
        "        alpha=0.2,\n",
        "        label=f\"{label} variance over {npoints_to_average} steps\",\n",
        "    )\n"
      ],
      "metadata": {
        "id": "HoWDUsB6j9qb"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import typing\n",
        "import time\n",
        "import collections\n",
        "import pathlib\n",
        "\n",
        "def compute_loss_and_accuracy(\n",
        "        dataloader: torch.utils.data.DataLoader,\n",
        "        model: torch.nn.Module,\n",
        "        loss_criterion: torch.nn.modules.loss._Loss):\n",
        "    \"\"\"\n",
        "    Computes the average loss and the accuracy over the whole dataset\n",
        "    in dataloader.\n",
        "    Args:\n",
        "        dataloder: Validation/Test dataloader\n",
        "        model: torch.nn.Module\n",
        "        loss_criterion: The loss criterion, e.g: torch.nn.CrossEntropyLoss()\n",
        "    Returns:\n",
        "        [average_loss, accuracy]: both scalar.\n",
        "    \"\"\"\n",
        "    average_loss = 0\n",
        "    accuracy = 0\n",
        "    #loss_fn = torch.nn.MSELoss(reduction='elementwise_mean')\n",
        "    # TODO: Implement this function (Task  2a)\n",
        "    running_loss = 0\n",
        "    running_acc = 0\n",
        "    with torch.no_grad():\n",
        "        for (X_batch, Y_batch) in dataloader:\n",
        "            #print(X_batch, Y_batch)\n",
        "            # Transfer images/labels to GPU VRAM, if possible\n",
        "            X_batch = to_cuda(X_batch)\n",
        "            Y_batch = to_cuda(Y_batch)\n",
        "            # Forward pass the images through our model\n",
        "            output_probs = model(X_batch)\n",
        "            #print(output_probs.size(), Y_batch.size())\n",
        "\n",
        "            # loss = 0\n",
        "            # for i in range(output_probs.size(0)):\n",
        "            #     loss += loss_criterion(output_probs[i], Y_batch[i])\n",
        "            #print(output_probs)\n",
        "            #print(Y_batch)\n",
        "\n",
        "            loss = loss_criterion(output_probs, Y_batch)\n",
        "            running_loss+=loss\n",
        "\n",
        "\n",
        "            #average_loss = loss/output_probs.size(0)\n",
        "            # Compute Loss and Accuracy\n",
        "            #loss = (output_probs - Y_batch).pow(2).sum()\n",
        "            # print(output_probs.size(1), Y_batch.size(0))\n",
        "            # output_probs, _ = torch.max(output_probs, dim=1)\n",
        "            # average_loss = loss_fn(output_probs, Y_batch)\n",
        "\n",
        "            #accuracy:\n",
        "            #accuracy_funct = MulticlassAccuracy(num_classes=10)\n",
        "            #accuracy = accuracy_funct(output_probs, Y_batch)\n",
        "            #print('batch is this', X_batch.size())\n",
        "\n",
        "            accuracy2 = torch.sum(torch.argmax(output_probs, dim=1) == Y_batch)/X_batch.size(0)\n",
        "            running_acc += accuracy2\n",
        "    avg_loss = running_loss / len(dataloader)\n",
        "    avg_acc = running_acc / len(dataloader)\n",
        "\n",
        "            # Predicted class is the max index over the column dimension\n",
        "    return avg_loss, avg_acc\n",
        "\n",
        "\n",
        "class Trainer:\n",
        "\n",
        "    def __init__(self,\n",
        "                 batch_size: int,\n",
        "                 learning_rate: float,\n",
        "                 early_stop_count: int,\n",
        "                 epochs: int,\n",
        "                 model: torch.nn.Module,\n",
        "                 dataloaders: typing.List[torch.utils.data.DataLoader]):\n",
        "        \"\"\"\n",
        "            Initialize our trainer class.\n",
        "        \"\"\"\n",
        "        self.batch_size = batch_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.early_stop_count = early_stop_count\n",
        "        self.epochs = epochs\n",
        "\n",
        "        # Since we are doing multi-class classification, we use CrossEntropyLoss\n",
        "        self.loss_criterion = torch.nn.CrossEntropyLoss()\n",
        "        # Initialize the model\n",
        "        self.model = model\n",
        "        # Transfer model to GPU VRAM, if possible.\n",
        "        self.model = to_cuda(self.model)\n",
        "        # print(self.model)\n",
        "\n",
        "        # Define our optimizer. SGD = Stochastich Gradient Descent\n",
        "        self.optimizer = torch.optim.SGD(self.model.parameters(), self.learning_rate, weight_decay=0.001)\n",
        "\n",
        "        # Load our dataset\n",
        "        self.dataloader_train, self.dataloader_val, self.dataloader_test = dataloaders\n",
        "\n",
        "        # Validate our model everytime we pass through 50% of the dataset\n",
        "        print(len(self.dataloader_train))\n",
        "        self.num_steps_per_val = len(self.dataloader_train) / 2\n",
        "        self.global_step = 0\n",
        "        self.start_time = time.time()\n",
        "\n",
        "        # Tracking variables\n",
        "        self.train_history = dict(\n",
        "            loss=collections.OrderedDict(),\n",
        "            accuracy=collections.OrderedDict()\n",
        "\n",
        "        )\n",
        "        self.validation_history = dict(\n",
        "            loss=collections.OrderedDict(),\n",
        "            accuracy=collections.OrderedDict()\n",
        "        )\n",
        "        self.checkpoint_dir = pathlib.Path(\"checkpoints\")\n",
        "\n",
        "    def validation_step(self):\n",
        "        \"\"\"\n",
        "            Computes the loss/accuracy for all three datasets.\n",
        "            Train, validation and test.\n",
        "        \"\"\"\n",
        "        self.model.eval()\n",
        "        validation_loss, validation_acc = compute_loss_and_accuracy(\n",
        "            self.dataloader_val, self.model, self.loss_criterion\n",
        "        )\n",
        "        self.validation_history[\"loss\"][self.global_step] = validation_loss\n",
        "        self.validation_history[\"accuracy\"][self.global_step] = validation_acc\n",
        "        used_time = time.time() - self.start_time\n",
        "        print(\n",
        "            f\"Epoch: {self.epoch:>1}\",\n",
        "            f\"Batches per seconds: {self.global_step / used_time:.2f}\",\n",
        "            f\"Global step: {self.global_step:>6}\",\n",
        "            f\"Validation Loss: {validation_loss:.2f}\",\n",
        "            f\"Validation Accuracy: {validation_acc:.3f}\",\n",
        "            sep=\", \")\n",
        "        self.model.train()\n",
        "\n",
        "    def should_early_stop(self):\n",
        "        \"\"\"\n",
        "            Checks if validation loss doesn't improve over early_stop_count epochs.\n",
        "        \"\"\"\n",
        "        # Check if we have more than early_stop_count elements in our validation_loss list.\n",
        "        val_loss = self.validation_history[\"loss\"]\n",
        "        if len(val_loss) < self.early_stop_count:\n",
        "            return False\n",
        "        # We only care about the last [early_stop_count] losses.\n",
        "        relevant_loss = list(val_loss.values())[-self.early_stop_count:]\n",
        "        first_loss = relevant_loss[0]\n",
        "        # if first_loss == min(relevant_loss):\n",
        "        #     print(\"Early stop criteria met\")\n",
        "        #     return True\n",
        "        return False\n",
        "\n",
        "    def train_step(self, X_batch, Y_batch):\n",
        "        \"\"\"\n",
        "        Perform forward, backward and gradient descent step here.\n",
        "        The function is called once for every batch (see trainer.py) to perform the train step.\n",
        "        The function returns the mean loss value which is then automatically logged in our variable self.train_history.\n",
        "\n",
        "        Args:\n",
        "            X: one batch of images\n",
        "            Y: one batch of labels\n",
        "        Returns:\n",
        "            loss value (float) on batch\n",
        "        \"\"\"\n",
        "        # X_batch is the CIFAR10 images. Shape: [batch_size, 3, 32, 32]\n",
        "        # Y_batch is the CIFAR10 image label. Shape: [batch_size]\n",
        "        # Transfer images / labels to GPU VRAM, if possible\n",
        "        X_batch = to_cuda(X_batch)\n",
        "        Y_batch = to_cuda(Y_batch)\n",
        "\n",
        "        # Perform the forward pass\n",
        "        predictions = self.model(X_batch)\n",
        "        correct = torch.sum(Y_batch == torch.argmax(predictions, dim=1)).item()\n",
        "        self.running_acc += correct/X_batch.size(0)\n",
        "        # Compute the cross entropy loss for the batch\n",
        "        loss = self.loss_criterion(predictions, Y_batch)\n",
        "        self.running_loss += loss.item()\n",
        "        # Backpropagation\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        # Gradient descent step\n",
        "        self.optimizer.step()\n",
        "        # Reset all computed gradients to 0\n",
        "\n",
        "        return loss.detach().cpu().item()\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"\n",
        "        Trains the model for [self.epochs] epochs.\n",
        "        \"\"\"\n",
        "        def should_validate_model():\n",
        "            return self.global_step % self.num_steps_per_val == 0\n",
        "\n",
        "        self.running_acc = 0\n",
        "        self.running_loss = 0\n",
        "        for epoch in range(self.epochs):\n",
        "            self.epoch = epoch\n",
        "            # Perform a full pass through all the training samples\n",
        "            for X_batch, Y_batch in self.dataloader_train:\n",
        "                loss = self.train_step(X_batch, Y_batch)\n",
        "                self.train_history[\"loss\"][self.global_step] = loss\n",
        "                self.global_step += 1\n",
        "                # Compute loss/accuracy for validation set\n",
        "                if should_validate_model():\n",
        "                    self.validation_step()\n",
        "                    self.save_model()\n",
        "                    if self.should_early_stop():\n",
        "                        print(\"Early stopping.\")\n",
        "                        return\n",
        "\n",
        "    def save_model(self):\n",
        "        def is_best_model():\n",
        "            \"\"\"\n",
        "                Returns True if current model has the lowest validation loss\n",
        "            \"\"\"\n",
        "            val_loss = self.validation_history[\"loss\"]\n",
        "            validation_losses = list(val_loss.values())\n",
        "            return validation_losses[-1] == min(validation_losses)\n",
        "\n",
        "        state_dict = self.model.state_dict()\n",
        "        filepath = self.checkpoint_dir.joinpath(f\"{self.global_step}.ckpt\")\n",
        "\n",
        "        save_checkpoint(state_dict, filepath, is_best_model())\n",
        "\n",
        "    def load_best_model(self):\n",
        "        state_dict = load_best_checkpoint(self.checkpoint_dir)\n",
        "        if state_dict is None:\n",
        "            print(\n",
        "                f\"Could not load best checkpoint. Did not find under: {self.checkpoint_dir}\")\n",
        "            return\n",
        "        self.model.load_state_dict(state_dict)"
      ],
      "metadata": {
        "id": "YViUSqihkEo5"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets\n",
        "from torchvision.transforms import v2\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import torch\n",
        "import typing\n",
        "import numpy as np\n",
        "import pathlib\n",
        "np.random.seed(0)\n",
        "\n",
        "mean = (0.5, 0.5, 0.5)\n",
        "std = (.25, .25, .25)\n",
        "\n",
        "\n",
        "def get_data_dir():\n",
        "    server_dir = pathlib.Path(\"/work/datasets/cifar10\")\n",
        "    if server_dir.is_dir():\n",
        "        return str(server_dir)\n",
        "    return \"data/cifar10\"\n",
        "\n",
        "\n",
        "def load_cifar10(batch_size: int, validation_fraction: float = 0.1\n",
        "                 ) -> typing.List[torch.utils.data.DataLoader]:\n",
        "    # Note that transform train will apply the same transform for\n",
        "    # validation!\n",
        "    transform_train = v2.Compose([\n",
        "        # v2.RandomResizedCrop(size=(36,36)),\n",
        "        # v2.RandomHorizontalFlip(p=0.5),\n",
        "        # v2.RandomRotation(degrees=30),\n",
        "        #v2.RandomAffine(degrees=0, translate=(0.3, 0.3), scale=(0.3, 0.3)),\n",
        "        v2.ToTensor(),\n",
        "        v2.Normalize(mean, std)\n",
        "    ])\n",
        "    transform_test = v2.Compose([\n",
        "        v2.ToTensor(),\n",
        "        v2.Normalize(mean, std)\n",
        "    #     v2.ToDtype(torch.float32, scale=True),\n",
        "    #     v2.Normalize(mean=mean, std=std)\n",
        "    ])\n",
        "    data_train = datasets.CIFAR10(get_data_dir(),\n",
        "                                  train=True,\n",
        "                                  download=True,\n",
        "                                  transform=transform_train)\n",
        "\n",
        "    data_test = datasets.CIFAR10(get_data_dir(),\n",
        "                                 train=False,\n",
        "                                 download=True,\n",
        "                                 transform=transform_test)\n",
        "\n",
        "\n",
        "    print(len(data_train))\n",
        "    indices = list(range(len(data_train)))\n",
        "    split_idx = int(np.floor(validation_fraction * len(data_train)))\n",
        "\n",
        "    val_indices = np.random.choice(indices, size=split_idx, replace=False)\n",
        "    train_indices = list(set(indices) - set(val_indices))\n",
        "\n",
        "    train_sampler = SubsetRandomSampler(train_indices)\n",
        "    validation_sampler = SubsetRandomSampler(val_indices)\n",
        "\n",
        "    dataloader_train = torch.utils.data.DataLoader(data_train,\n",
        "                                                   sampler=train_sampler,\n",
        "                                                   batch_size=batch_size,\n",
        "                                                   num_workers=2,\n",
        "                                                   drop_last=True)\n",
        "\n",
        "    dataloader_val = torch.utils.data.DataLoader(data_train,\n",
        "                                                 sampler=validation_sampler,\n",
        "                                                 batch_size=batch_size,\n",
        "                                                 num_workers=2)\n",
        "\n",
        "    dataloader_test = torch.utils.data.DataLoader(data_test,\n",
        "                                                  batch_size=batch_size,\n",
        "                                                  shuffle=False,\n",
        "                                                  num_workers=2)\n",
        "\n",
        "    return dataloader_train, dataloader_val, dataloader_test"
      ],
      "metadata": {
        "id": "85NeVzvllbbc"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "-dAw1lG-iVjV",
        "outputId": "0d625bf9-1748-4bde-a9cb-cecb4cc3282f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "50000\n",
            "703\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-3503f4c7a584>\u001b[0m in \u001b[0;36m<cell line: 132>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-6-3503f4c7a584>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stop_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     )\n\u001b[0;32m--> 128\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m     \u001b[0mcreate_plots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"task2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-71b3b917f1f1>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0;31m# Perform a full pass through all the training samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataloader_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_history\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-71b3b917f1f1>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, X_batch, Y_batch)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;31m# Perform the forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_batch\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_acc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcorrect\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-3503f4c7a584>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;31m# TODO: Implement this function (Task  2a)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;31m#print(x.size())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;31m#print(self._calculate_num_output_features(3,32,32))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;31m#print(x.size())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/pooling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m         return F.max_pool2d(input, self.kernel_size, self.stride,\n\u001b[0m\u001b[1;32m    167\u001b[0m                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mceil_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m                             return_indices=self.return_indices)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_jit_internal.py\u001b[0m in \u001b[0;36mfn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    486\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mif_true\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mif_false\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mif_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mif_false\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36m_max_pool2d\u001b[0;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[1;32m    789\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstride\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[0mstride\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 791\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mceil_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    792\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import pathlib\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class ExampleModel(nn.Module):\n",
        "    def __init__(self, image_channels, num_classes):\n",
        "        \"\"\"\n",
        "        Is called when model is initialized.\n",
        "        Args:\n",
        "            image_channels. Number of color channels in image (3)\n",
        "            num_classes: Number of classes we want to predict (10)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # TODO: Implement this function (Task  2a)\n",
        "        num_filters = 64  # Set number of filters in first conv layer\n",
        "        self.num_classes = num_classes\n",
        "        torch.manual_seed(50)\n",
        "        # Define the convolutional layers\n",
        "        self.feature_extractor = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=image_channels,\n",
        "                out_channels=num_filters,\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=2,\n",
        "            ),\n",
        "            nn.BatchNorm2d(num_filters),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(in_channels=num_filters, out_channels=80, kernel_size=4, stride=1, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            #nn.BatchNorm2d(80),\n",
        "            nn.Conv2d(in_channels=80, out_channels=128, kernel_size=6, stride=1, padding=2),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "        # The output of feature_extractor will be [batch_size, num_filters, 16, 16]\n",
        "        self.num_output_features = 2048 #32 * 32 * 32 #128*4*4\n",
        "        # Initialize our last fully connected layer\n",
        "        # Inputs all extracted features from the convolutional layers\n",
        "        # Outputs num_classes predictions, 1 for each class.\n",
        "        # There is no need for softmax activation function, as this is\n",
        "        # included with nn.CrossEntropyLoss\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(self.num_output_features, 64),\n",
        "            nn.ReLU(),\n",
        "            #nn.Dropout(p=0.2),\n",
        "            nn.Linear(64, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Performs a forward pass through the model\n",
        "        Args:\n",
        "            x: Input image, shape: [batch_size, 3, 32, 32]\n",
        "        \"\"\"\n",
        "        # TODO: Implement this function (Task  2a)\n",
        "        #print(x.size())\n",
        "        x = self.feature_extractor(x)\n",
        "        #print(self._calculate_num_output_features(3,32,32))\n",
        "        #print(x.size())\n",
        "        #x = nn.Flatten(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def _calculate_num_output_features(self, image_channels, input_size1, input_size2):\n",
        "        # Create a dummy input tensor to get the output shape\n",
        "        x = torch.zeros((1, image_channels, input_size1, input_size2))\n",
        "        output_shape = self.feature_extractor(x).shape\n",
        "        return output_shape[1] * output_shape[2] * output_shape[3]\n",
        "\n",
        "\n",
        "\n",
        "        # batch_size = x.shape[0]\n",
        "        # out = x\n",
        "        # expected_shape = (batch_size, self.num_classes)\n",
        "        # assert out.shape == (\n",
        "        #     batch_size,\n",
        "        #     self.num_classes,\n",
        "        # ), f\"Expected output of forward pass to be: {expected_shape}, but got: {out.shape}\"\n",
        "        # return out\n",
        "\n",
        "\n",
        "def create_plots(trainer: Trainer, name: str):\n",
        "    plot_path = pathlib.Path(\"plots\")\n",
        "    plot_path.mkdir(exist_ok=True)\n",
        "    # Save plots and show them\n",
        "    plt.figure(figsize=(20, 8))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.title(\"Cross Entropy Loss\")\n",
        "    plot_loss(\n",
        "        trainer.train_history[\"loss\"], label=\"Training loss\", npoints_to_average=10\n",
        "    )\n",
        "    plot_loss(trainer.validation_history[\"loss\"], label=\"Validation loss\")\n",
        "    plt.legend()\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.title(\"Accuracy\")\n",
        "    plot_loss(trainer.validation_history[\"accuracy\"], label=\"Validation Accuracy\")\n",
        "    plt.legend()\n",
        "    plt.savefig(plot_path.joinpath(f\"{name}_plot.png\"))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Set the random generator seed (parameters, shuffling etc).\n",
        "    # You can try to change this and check if you still get the same result!\n",
        "    set_seed(0)\n",
        "    print(f\"Using device: {get_device()}\")\n",
        "    epochs = 10\n",
        "    batch_size = 64\n",
        "    learning_rate = 5e-2\n",
        "    early_stop_count = 4\n",
        "    dataloaders = load_cifar10(batch_size)\n",
        "    model = ExampleModel(image_channels=3, num_classes=10)\n",
        "    trainer = Trainer(\n",
        "        batch_size, learning_rate, early_stop_count, epochs, model, dataloaders\n",
        "    )\n",
        "    trainer.train()\n",
        "    create_plots(trainer, \"task2\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ]
}